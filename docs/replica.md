# Репликация
Репликация в ClickHouse и других базах данных используется для повышения отказоустойчивости и доступности данных.

***От чего защищает репликация:***

1. **Отказ одного или нескольких узлов:**
    - При падении одного или нескольких узлов с репликами (например, из-за аппаратного сбоя) другие узлы с репликами смогут продолжать обслуживать запросы. Это обеспечивает высокую доступность (High Availability, HA) системы.

2. **Отказ дисков и повреждение оборудования:**
    - Если данные на диске повреждаются или диск выходит из строя, другая реплика может продолжать функционировать и принимать запросы на чтение/запись, сохраняя консистентность данных.

3. **Обновления и обслуживание:**
    - Репликация позволяет проводить обновления и обслуживание узлов по очереди, не приводя к недоступности системы в целом.

4. **Балансировка нагрузки:**
    - Репликация помогает распределять нагрузку на чтение, так как запросы могут выполняться на разных репликах, что снижает нагрузку на каждый отдельный узел.

5. **Failover (переключение на другую реплику):**
    - В случае проблем с основным узлом, система может автоматически переключиться на реплику, минимизируя простои и обеспечивая бесперебойную работу.

### Взаимодействие компонентов
Для хранения и координации действий репликации в ClickHouse требуется распределённое хранилище данных, которое может гарантировать согласованность состояния. Для этого ClickHouse использует один из двух вариантов: Zookeeper или ClickHouse Keeper.

### Как организовать кластер ClickHouse?

#### 1. **Шардирование + Репликация**
Наиболее эффективный способ организации кластера ClickHouse — это использование шардирования и репликации. Этот подход позволяет вам распределить нагрузку между узлами и обеспечивать отказоустойчивость.

- **Шардирование**: Каждая таблица разбивается на отдельные части (шарды), которые хранятся на разных узлах кластера.
- **Репликация**: Для каждого шарда создаются реплики, которые содержат те же данные, обеспечивая доступность данных даже при падении одного или нескольких узлов.

**Недостаток**: Для полной реализации репликации и шардирования потребуется в два раза больше машин, так как каждая реплика шарда должна находиться на отдельном узле. Однако этот недостаток можно нивелировать, используя контейнеризацию в Kubernetes.

**Пример с Kubernetes**:
- Есть два узла, выделенных под ClickHouse в Kubernetes кластере.
- На каждом из них развёрнуто по два контейнера:
  1. На первом узле контейнер с шардом 1 (`clickhouse-01-01`) и контейнер с репликой шарда 2 (`clickhouse-02-02`).
  2. На втором узле контейнер с шардом 2 (`clickhouse-02-01`) и контейнер с репликой шарда 1 (`clickhouse-01-02`).

В таком случае при потере одного узла кластер останется работоспособным, так как каждый шард будет представлен хотя бы одной репликой. При нормальной работе нагрузка распределяется равномерно между двумя узлами.

#### 2. **Keeper**
Важной частью кластера является система координации — **Keeper** (или **ZooKeeper**), которая необходима для корректного взаимодействия между репликами и шардами. Если кворум Keeper нарушен, кластер перейдет в режим `readOnly`, даже если шард и реплики работают корректно. Поэтому Keeper рекомендуется развёртывать на отдельных машинах с SSD-дисками для высокой производительности.

**Рекомендации по Keeper**:
- Используйте минимум 3 узла (рекомендуется 5 или 7 для обеспечения кворума).
- Не удаляйте данные из Keeper вручную.
- Регулярно делайте бэкап данных из Keeper.
- Регулярно обновляйте версии ClickHouse и Keeper для использования последних улучшений и исправлений.

### Создание таблиц и управление данными

ClickHouse автоматически выполняет фоновые задачи по слиянию и вставке данных, используя механизм **MergeTree**.

- Настройте количество потоков для фоновых задач через параметр:
  ```xml
  <background_schedule_pool_size>16</background_schedule_pool_size>
  ```

- **Операции DDL** (создание и изменение таблиц) также координируются через механизм репликации при использовании команды `ON CLUSTER`. Однако такие команды, как `CREATE`, `DROP`, `DETACH`, `ATTACH` и другие, являются локальными и должны выполняться вручную на каждом узле.

### Переход с MergeTree на ReplicatedMergeTree

Если вам нужно перейти с обычной таблицы **MergeTree** на её реплицируемую версию **ReplicatedMergeTree**, существует два подхода:

1. **Ручной способ**:
   - Создайте реплицируемую таблицу с таким же движком и структурой.
   - Используйте команду `ATTACH` для присоединения каждой партиции по отдельности.

2. **Автоматический способ** (в версиях 24.2 и выше):
   - Добавьте флаг репликации в каталог таблицы, и ClickHouse автоматически преобразует таблицу в реплицируемую.

### Ключевые моменты для обеспечения отказоустойчивости

1. **Отказоустойчивость с помощью шардирования и репликации**: Правильное распределение шардов и реплик по узлам кластера обеспечит продолжение работы кластера даже при отказе одного из узлов.
2. **Кворумные вставки и чтения**: Используйте параметры для контроля кворума на вставку данных (`insert_quorum`) и чтение с доступных реплик (`prefer_localhost_replica`), чтобы гарантировать консистентность данных.
3. **Автоматическая синхронизация**: В случае восстановления реплик механизм **MergeTree** автоматически синхронизирует данные и выполнит все недостающие слияния.

# Настройки для репликации

Для обеспечения отказоустойчивости в кластере ClickHouse с двумя шардами и двумя репликами на каждый шард, необходимо настроить ряд параметров. Вот основные настройки и шаги, которые нужно рассмотреть:

## 1. insert_distributed_sync

Установка значения `<insert_distributed_sync>0</insert_distributed_sync>` в конфигурации ClickHouse приводит к **асинхронной вставке** в распределённые таблицы, что означает, что запрос на вставку возвращает управление клиенту **немедленно**, не дожидаясь завершения вставки на всех репликах. Давайте рассмотрим, как это работает и какие у этого преимущества и недостатки.

### Как это работает:
- **Асинхронная вставка**: При вставке в распределённую таблицу ClickHouse сразу же возвращает успешный ответ клиенту, даже если данные еще не полностью распределены по всем шардам и репликам.
- **Очередь вставки**: Данные сначала записываются на локальную ноду, и затем ClickHouse самостоятельно распределяет их по другим шардам и репликам в фоновом режиме.
- **Нет гарантии синхронной записи на все реплики**: Поскольку вставка асинхронна, данные могут некоторое время не быть доступны на некоторых репликах.

### Преимущества:

1. **Быстрая обратная связь клиенту**:
   - Основное преимущество асинхронной вставки в том, что клиент сразу получает ответ, не дожидаясь окончания записи на все реплики. Это снижает общую задержку и ускоряет обработку массовых операций записи, что полезно в сценариях с большими объемами данных.
   
2. **Улучшенная производительность вставок**:
   - Асинхронная вставка может снизить нагрузку на систему за счет того, что она не требует ждать окончания вставки на все реплики, что улучшает общую производительность системы.

3. **Устойчивость к временным сбоям сети**:
   - Если одна или несколько реплик временно недоступны, это не блокирует вставку. Данные будут отправлены на недоступные реплики, как только они станут доступными, за счет очереди репликации.

### Недостатки:

1. **Неполные данные на репликах**:
   - Если данные еще не распределены на все реплики, запросы на чтение с этих реплик могут не увидеть недавно вставленные данные. Это может привести к неактуальным результатам при распределенных запросах до завершения фоново распределения данных.

2. **Потеря данных при сбое**:
   - Если нода, на которую данные сначала вставляются локально, выйдет из строя до того, как данные успеют быть переданы на другие реплики, есть риск потери данных. Хотя ClickHouse использует механизм репликации и кворум для вставок, в асинхронном режиме такой механизм может не сработать вовремя.

3. **Осложнение диагностики и отладки**:
   - Асинхронная вставка может усложнить отладку, так как при возникновении проблем с распределением данных на реплики ошибки могут проявляться позже, что требует отслеживания логов и статусов репликации.

4. **Задержки при чтении с некоторых реплик**:
   - Если реплики отстают по синхронизации, запросы на чтение с таких реплик могут возвращать неполные или устаревшие данные.

### Сценарии, где это полезно:

1. **Массовые вставки данных**:
   - Асинхронная вставка хорошо подходит для ситуаций, где важно быстро записывать большие объемы данных, например, при сборе логов, метрик или другой телеметрии.

2. **Нагрузочные тесты**:
   - Асинхронная вставка может быть полезной для нагрузочных тестов, где важна высокая пропускная способность при вставках данных.

### Сценарии, где это может вызвать проблемы:

1. **Чувствительные к задержкам чтения системы**:
   - В системах, где важно мгновенно видеть все вставленные данные на всех репликах (например, финансовые приложения, мониторинг в реальном времени), асинхронная вставка может привести к проблемам с целостностью данных.

2. **Критичные вставки с высокой доступностью**:
   - В ситуациях, где важно гарантировать, что данные реплицируются синхронно на все узлы перед тем, как продолжить выполнение запроса, асинхронный подход может быть рискованным.

### Заключение:
- **Преимущества**: Асинхронная вставка ускоряет операции вставки и снижает нагрузку на кластер, что особенно полезно для систем с большим объемом данных.
- **Недостатки**: Возможные проблемы с консистентностью данных на репликах, а также риск потери данных при сбоях.

## 2. **Включение внутренней репликации (`internal_replication`)**

Механизм **внутренней репликации** в ClickHouse основан на движке таблиц `ReplicatedMergeTree`. Внутренняя репликация (internal replication) обеспечивает автоматическую синхронизацию данных между несколькими репликами внутри шарда в кластере, гарантируя, что все реплики хранят одинаковые данные и могут использоваться для обработки запросов в случае отказа одной из реплик.


### Настройка и подключение

#### Конфигурация `<remote_servers>` для внутренней репликации

Для корректной работы внутренней репликации необходимо настроить кластер в конфигурационном файле ClickHouse, включив параметр `internal_replication`:

```xml
<remote_servers>
    <cluster_name>
        <shard>
            <internal_replication>true</internal_replication>
            <replica>
                <host>clickhouse01</host>
                <port>9000</port>
            </replica>
            <replica>
                <host>clickhouse02</host>
                <port>9000</port>
            </replica>
        </shard>
        ...
    </cluster_name>
</remote_servers>
```

- `<internal_replication>true</internal_replication>` — включает автоматическую репликацию внутри шарда.
- Каждая реплика в кластере должна иметь свой хост и порт для подключения.

#### Конфигурация таблиц в Keeper

Таблицы, использующие механизм внутренней репликации, должны быть зарегистрированы в Keeper под соответствующими путями. Пример правильной структуры каталогов в Keeper:

```bash
/ :) ls "/clickhouse/tables/cluster_2S_2R"
/shard01
/shard02

/ :) ls "/clickhouse/tables/cluster_2S_2R/shard02/"
/events
```

- Каждая таблица должна быть зарегистрирована для каждого шарда (например, `shard01`, `shard02`) в кластере.
- Внутри каждого шарда должно быть указано имя таблицы, которое будет синхронизироваться между репликами.

#### Конфигурация макросов

Чтобы упростить настройку для разных реплик и шардов, используется механизм макросов:

```xml
<clickhouse>
    <macros>
        <shard from_env="CH_SHARD"></shard>
        <replica from_env="HOSTNAME"></replica>
        <cluster>cluster_2S_2R</cluster>
    </macros>
</clickhouse>
```

- `CH_SHARD` — переменная окружения для указания номера шарда.
- `HOSTNAME` — переменная окружения для указания имени текущей реплики.
- Макросы помогают упростить конфигурацию кластера, сделав её универсальной для всех шардов и реплик.

#### Отказоустойчивость и синхронизация данных

При отказе одной или обеих реплик шарда кластер продолжает работу, сохраняя данные на локальном диске активных реплик. Пример расположения данных на диске:

```bash
/var/lib/clickhouse/store/3df/3df13b2e-eca5-42c2-a154-f3ff401a2592:
shard2_all_replicas
```

Когда хотя бы одна из реплик становится доступной, данные, записанные на активных репликах, автоматически синхронизируются. Механизм восстановления работает следующим образом:

1. **Данные записываются на локальный диск активных реплик.**
2. **Как только реплика становится доступной**, она получает недостающие данные от других реплик через механизм `FETCH PART`.
3. **После восстановления всех реплик** они загружают данные от активной реплики, чтобы синхронизировать своё состояние с остальными.

Этот процесс происходит асинхронно, но гарантирует, что данные на всех репликах будут синхронизированы.

### Как работает внутренняя репликация в ClickHouse:

1. **Создание и настройка реплицируемых таблиц**:
   Таблицы, использующие внутреннюю репликацию, создаются на основе движков семейства `ReplicatedMergeTree`. Эти таблицы должны быть созданы с указанием уникального пути для каждой реплики, который хранится в Keeper.

   Пример создания таблицы:
   ```sql
   CREATE TABLE company_db.events
   (
       id UInt32,
       event_type String,
       event_time DateTime
   )
   ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/events', '{replica}')
   ORDER BY id;
   ```
   - `/clickhouse/tables/{shard}/events`: путь в Keeper, где хранится информация об этой таблице для конкретного шарда.
   - `{replica}`: уникальный идентификатор реплики.

2. **Keeper как координационный сервис**:
   Keeper координирует работу реплик и следит за состоянием каждой реплики. Он хранит метаданные о таблицах, таких как активные сегменты данных (parts), текущие состояния реплик и логи операций.

   - Каждая реплика имеет свой каталог в Keeper (например, `/clickhouse/tables/{shard}/events/replicas/clickhouse01`).
   - В момент создания или изменения данных через запросы INSERT, информация о добавленных сегментах данных записывается в Keeper.
   - Каждая реплика отслеживает изменения в логах Keeper и "подтягивает" недостающие данные у других реплик.

3. **Механизм репликации данных**:
   Репликация основана на принципе обмена сегментами данных (`parts`) между репликами через координацию с помощью Keeper:
   - При выполнении запроса `INSERT` на одной из реплик данные сначала записываются на локальный диск этой реплики как часть (part).
   - После этого информация об этом новом сегменте данных отправляется в Keeper, который фиксирует, что этот сегмент добавлен на одной из реплик.
   - Другие реплики следят за логами в Keeper и видят, что появился новый сегмент данных, которого у них нет. Они загружают недостающие сегменты с реплики, которая их уже имеет, и записывают их на свой диск.
   - Это асинхронный процесс, поэтому каждая реплика может находиться на разных этапах синхронизации, но в итоге все реплики будут иметь одинаковые данные.

4. **Механизм чтения данных**:
   Чтение данных возможно с любой активной реплики, так как каждая реплика хранит полную копию данных. Загрузчик данных `MergeTree` работает с локальными сегментами данных на реплике. В случае отказа одной из реплик другие реплики продолжат обслуживать запросы.

5. **Балансировка и распределение нагрузки**:
   Внутренняя репликация позволяет распределять нагрузку между репликами. Например, запросы на чтение могут выполняться на любой реплике (если она активна), а запросы на запись будут автоматически реплицированы на все реплики через Keeper.

6. **Синхронизация и устранение конфликтов**:
   Репликация в ClickHouse гарантирует, что даже если одна реплика временно отключается или теряет соединение с кластером, она сможет догнать и синхронизироваться с другими репликами после восстановления. Реплика просто скачает недостающие сегменты данных и применит их.

7. **Механизм слияния сегментов (Merge)**:
   Данные в ClickHouse хранятся в виде частей (parts), и каждая реплика независимо выполняет операции слияния (merge). Это необходимо для того, чтобы оптимизировать хранение данных и уменьшить количество отдельных частей.

### Взаимодействие между репликами:

1. **Логирование операций**:
   Вся информация о записи новых данных, удалениях, изменениях фиксируется в логах, которые хранятся в Keeper. Все реплики читают эти логи и синхронизируются друг с другом, применяя изменения.

2. **Загрузка недостающих сегментов (fetch)**:
   Если на одной из реплик не хватает какого-либо сегмента данных, она автоматически скачает его у других реплик через специальный механизм `FETCH PART`. Keeper координирует, какая реплика передаст недостающий сегмент.

3. **Проверка целостности данных**:
   Каждая реплика периодически проверяет целостность своих данных и сверяется с другими репликами, чтобы убедиться, что все данные находятся в актуальном состоянии.

### Возможные проблемы и их решения:

1. **Проблемы с сетью**:
   Если одна из реплик теряет связь с Keeper или другими репликами, она становится `readonly` (только для чтения), пока соединение не восстановится.

2. **Конфликты данных**:
   Конфликты практически исключены, так как каждая операция имеет свой идентификатор, и Keeper отслеживает порядок их выполнения. Реплики могут восполнять недостающие операции асинхронно.

### Заключение:

- **Внутренняя репликация** в ClickHouse основана на синхронизации сегментов данных между репликами через Keeper.
- Основной механизм — это чтение логов из Keeper и загрузка недостающих данных.
- Реплики могут обрабатывать запросы на чтение независимо друг от друга, а данные записываются на все реплики через координацию с Keeper.
- Репликация в ClickHouse — это асинхронный процесс, который гарантирует, что в итоге все реплики будут иметь одинаковые данные.

## 3. **Настройка отказоустойчивости и кворума**

**Замечание** - Применять только если кол-во реплик 3 или больше.

### **Запись с кворумом**:
Для обеспечения надёжности данных, записываемых в кластер, следует включить кворумную запись. Это гарантирует, что данные будут сохранены на нескольких репликах, даже если одна или несколько реплик временно недоступны.

- **Минимум две реплики для кворума**:
  Включите кворумную запись, чтобы записанные данные гарантированно сохранялись как минимум на **двух репликах каждого шарда**:

  ```xml
  <insert_quorum>2</insert_quorum>
  <insert_quorum_timeout>60</insert_quorum_timeout>
  ```
  Это обеспечит, что данные будут вставлены хотя бы на две реплики. Если кворум не будет достигнут в течение 60 секунд, вставка будет считаться неудачной.

  > **Важно**: Если значение `<insert_quorum>` установлено в `1`, это эквивалентно обычной вставке без кворума. Данные сохраняются только на одной реплике, что снижает отказоустойчивость.

### **Чтение с одной реплики**:
В случаях, когда необходимо оптимизировать запросы на чтение, можно настроить приоритет чтения с одной доступной реплики. Это может быть полезно, когда одна из реплик временно недоступна:

- **Чтение с локальной реплики**:

  ```xml
  <prefer_localhost_replica>true</prefer_localhost_replica>
  ```

  Это позволит направлять запросы на чтение на локальную реплику, если она доступна, уменьшая нагрузку на другие реплики. Однако эта настройка не влияет на отказоустойчивость и может быть полезной в обычных условиях работы.

## 4. **Режим работы при сбоях (emergency mode)**
   Если падет несколько реплик, то необходимо, чтобы оставшиеся реплики продолжали обработку запросов на чтение и запись:

   - **Настройка распределенных таблиц**:
     Используйте настройки для работы с распределенными таблицами, которые будут позволять кластеру продолжать вставку данных и чтение с оставшихся реплик:

     ```sql
     CREATE TABLE distributed_table AS local_table
     ENGINE = Distributed(cluster_name, database_name, table_name, 'replica_1', 1);
     ```

     Это гарантирует, что даже в случае сбоя нескольких реплик, оставшиеся реплики продолжат получать данные и их обслуживать.

## 5. **Автоматическая синхронизация после восстановления**

### **Параметр `<max_replicated_merges_in_queue>`:**

Параметр `<max_replicated_merges_in_queue>` определяет максимальное количество слияний данных (merge operations), которые могут находиться в очереди для выполнения на реплике, использующей движок таблиц **ReplicatedMergeTree** в ClickHouse.

### **Как работает механизм MergeTree:**
- Внутренний механизм **MergeTree** используется для хранения данных и их автоматического слияния (merge) в более крупные файлы для оптимизации запросов и уменьшения фрагментации.
- В контексте репликации, **ReplicatedMergeTree** отслеживает, какие данные находятся на каждой реплике, и автоматически синхронизирует их при подключении восстановленной или отставшей реплики.
- Каждая реплика ведёт журнал операций (log), чтобы отслеживать свои действия по вставке данных, слиянию и удалению. Когда реплика временно становится недоступной и затем восстанавливается, она автоматически подтягивает недостающие операции из кластера и выполняет их для синхронизации с другими репликами.

### **Работа параметра `<max_replicated_merges_in_queue>`:**
- Когда реплика восстанавливается или была временно отключена, ей нужно синхронизироваться с остальными репликами. Это включает процесс слияния фрагментов данных (мерджей) в более крупные блоки.
- Параметр `<max_replicated_merges_in_queue>` регулирует, сколько таких слияний можно выполнить одновременно.

#### **Подробности работы:**
1. **Очередь репликации:** Когда реплика восстановлена, она подтягивает информацию о недостающих фрагментах данных и начинает выполнять слияния, чтобы данные между репликами синхронизировались.

2. **Ограничение количества операций:** Параметр задаёт верхний предел для количества параллельных слияний. По умолчанию это значение установлено в `32`. Это значит, что одновременно могут выполняться не более 32 операций слияния на одной реплике. Если на реплике есть более 32 операций, остальные операции будут помещены в очередь до завершения текущих.

3. **Производительность и балансировка:** Этот параметр помогает балансировать нагрузку на реплику, предотвращая ситуацию, когда слишком большое количество слияний может перегрузить систему (что особенно важно при восстановлении после отказа). Однако слишком низкое значение может замедлить процесс восстановления реплики, поскольку данные будут синхронизироваться медленнее.

4. **Автоматическая синхронизация:** После того, как восстановленная реплика подключается к кластеру, она автоматически начинает подтягивать недостающие данные, а затем выполняет слияния этих данных с уже существующими фрагментами, используя механизм **MergeTree**.

### **Практическое значение настройки:**
- В кластере с большим количеством данных и высокой нагрузкой на реплики стоит увеличить значение `<max_replicated_merges_in_queue>`, чтобы реплики могли быстрее выполнять слияния и синхронизировать данные.
- Однако, если ресурсы системы ограничены (например, по CPU или дисковому вводу-выводу), стоит использовать более консервативное значение, чтобы избежать перегрузки.

### **Пример настройки:**
```xml
<max_replicated_merges_in_queue>64</max_replicated_merges_in_queue>
```
Это увеличит количество одновременно выполняемых операций слияния до 64, что ускорит синхронизацию реплик, особенно в сценариях восстановления после отказов.

Таким образом, параметр `<max_replicated_merges_in_queue>` играет важную роль в оптимизации процессов восстановления и синхронизации данных в кластере ClickHouse, использующем ReplicatedMergeTree.

## 6. **Проверка состояния реплик и очереди репликации**
   Следует регулярно проверять таблицу `system.replication_queue`, чтобы убедиться, что очереди репликации накапливаются в случае падения реплик, а также что синхронизация данных происходит корректно после их восстановления.

   Для этого, нужно настроить мониторинг кластера.

### Заключение:
Эти настройки обеспечат:
- **Отказоустойчивость**: Кластер продолжит работать, даже если одна или две реплики (одна в каждом шарде) выйдут из строя.
- **Асинхронную репликацию**: Данные будут временно храниться локально при отсутствии одной из реплик, а затем синхронизированы.
- **Автоматическое восстановление**: После восстановления реплик, данные будут автоматически синхронизированы на них.

Этот подход позволит достичь высокой доступности и продолжить работу в аварийных ситуациях без сбоев в работе запросов.
