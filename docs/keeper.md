#### Keeper
Keeper не хранит в себе данные таблиц, но содержит метаданные, которые говорят, какие данные должны хранится в каждой реплике. Например, если вы вставляете 10 строк, то ClickHouse создаст парт и ссылка на этот парт будет хранится в Keeper. Все инстансы ClickHouse будут знать, что парт есть в кластере и необходимо найти его в локальной файловой системе. А если его нет локально, то инстанс должен обратиться к другим инстансам, чтобы скопировать его. Корректность метаданных и доступность самого Keeper критически важны для корректной работы репликации: если вы потеряете свой Keeper, то все реплицируемые таблицы уйдут в read-only режим до возвращения Keeper в рабочее состояние. Сейчас внутри ClickHouse есть функция SystemRestoreReplica, которая позволяет восстановить репликация из локальных данных, однако этот вариант стоит использовать только в крайнем случае. Помимо хранения метаданных, ClickHouse также использует Keeper для координации кластера.

!TODO ### Нужно уточнить информацию относительно использования keeper на каждой отдельной ноде. Если брать во внимание наш способ разворачивания кластера, то исмпользование keeper на первый взгляд предпочтительней. 
ClickHouse Keeper можно поддерживать в двух разных установках: в качестве обособленной инсталляции на отдельных серверах или как часть ClickHouse в виде Process Keeper — части бинарного файла ClickHouse clickhouse-server. Несмотря на то, что второй вариант кажется проще и дешевле (так как по сути Keeper уже встроен в ClickHouse), мы настоятельно рекомендуем использовать ClickHouse Keeper как отдельную инсталляцию, так как это повышает отказоустойчивость системы в целом.

<internal_replication>true</internal_replication>

#### ZooKeeper

Внутри Zookeeper все данные о кластере будут хранится в директории /clickhouse и в ней будет находятся следующее:

ddl — все DDL запросы, которые необходимо реплицировать и применять к репликам.
metadata — хранит часть информации о метаданных реплицируемой таблицы, например первичный ключ, ключ раздела и т. д;
columns — хранит информацию о полях реплицируемой таблицы;
block_numbers — хранит все разделы таблицы;
leader_election — используется для выбора мастера между репликами. При запросе сначала будет выбрана ведущая реплика, а затем на ее основе будет синхронизирована другая реплика. Несколько реплик могут быть лидерами одновременно;
log — каталог, служащий очередью задач для хранения операций над репликой таблицы;
blocks — хранит хэш-информацию блоков данных, записанных в таблицу за определенный период времени для дедупликации. Формат следующих дочерних узлов — partition_hash_hash;
mutation — каталог, служащий очередью задач для хранения операций мутации над реплицированной таблицей;
replicas/node_hostname — хранит информацию о репликах:
is_active — флаг, указывающий, активен ли инстанс или нет, если на сервере произошел сбой, узел не будет существовать, и будет добавлен заново после восстановления;
mutation_pointer — хранит следующую задачу в очереди мутаций, которую следует выполнить;
log_pointer — хранит следующую задачу в очереди журналов, которую следует выполнить;
is_lost — флаг, указывающий, устарела ли реплика, на основании того, обновлён ли указатель log_pointer, при этом 0 — нормально, -1 — устарела, а 1 — находится в процессе восстановления;
metadata — хранит информацию о метаданных таблицы, как и metadata выше;
columns — хранит информацию о столбцах таблицы;
parts — хранит информацию о партах таблицы, каждый блок информации содержит контрольные суммы и информацию о столбцах;
queue — временная очередь обработки
