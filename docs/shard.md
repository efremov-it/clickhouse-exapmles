# Шардирование

В некоторых случаях, нет необходимости создавать избыточные данные, репликации (в силу различных особенностей, такая необходимость может быть). Но нужно обеспечить горизонтальное масштабирование.
В данном случае прекрасно подхиодит решение только шардирования.

Но есть и ряд недостатков, которые с "коробки" могут вызвать определенные трудности при добавлении,удалении,потере одного из шардов.

Основной из них:
При потере шарда, выборка, вставка данных в доступные шарды невозможна!
Для этого, нужно добавить настройки в `users.xml`

```xml
<company>
    <profiles>
        <default>
            <send_timeout>5</send_timeout> <!-- 5 секунд на отправку. Для тежелых запросов лучше не менять -->
            <connect_timeout>2</connect_timeout> <!-- 2 секунды на прием -->
            <receive_timeout>3</receive_timeout> <!-- 3 секунды на подключение. Для тежелых запросов лучше не менять -->
            <skip_unavailable_shards>1</skip_unavailable_shards> <!-- Игнорируем недоступные шарды -->
            <insert_distributed_sync>0</insert_distributed_sync> <!-- Асинхронная вставка без ожидания -->
            <connect_timeout_with_failover_ms>750</connect_timeout_with_failover_ms> <!-- 0.75 сек на переключение -->
            <connections_with_failover_max_tries>2</connections_with_failover_max_tries> <!-- 2 попытки на подключение -->
        </default>
    </profiles>
</company>
```

Подробней о ключевых настройках:

- [skip_unavailable_shards](https://clickhouse.com/docs/en/operations/settings/settings#skip_unavailable_shards)

Когда этот параметр установлен в 1, ClickHouse пропускает недоступные шарды и продолжает выполнение запроса только с доступными шардами. Это предотвращает прерывание операций чтения или записи из-за временной недоступности одного или нескольких шардов в кластере.

### Как работает `skip_unavailable_shards`

- **Чтение (SELECT):**
  - Если один или несколько шардов становятся недоступны во время выполнения запроса `SELECT`, ClickHouse **продолжает выполнение** запроса только для доступных шардов.
  - Это позволяет системе избегать ошибок при чтении данных, когда часть шардов временно отключена или находится в процессе восстановления.
  - Однако, при таком подходе, данные с недоступных шардов **не будут возвращены** в результат запроса.

- **Запись (INSERT):**
  - Если один или несколько шардов недоступны во время выполнения операции вставки, данные **не будут отправлены на эти недоступные шарды**. 
  - В зависимости от того, как настроены другие параметры (например, `insert_distributed_sync`), данные могут либо быть буферизованы на локальной файловой системе и отправлены позже (при асинхронной вставке), либо операция завершится без попытки вставки на недоступные шарды.

### Поведение при недоступности одного шарда `insert_distributed_sync`

1. **`insert_distributed_sync = 1` (синхронная запись):**
   - При **недоступности любого шарда** запрос `INSERT` завершится **ошибкой**.
   - Данные **не будут буферизоваться** на локальном сервере. Вся операция будет прервана, и данные не будут вставлены на доступные шарды.
   - **Данные потеряются** только в том случае, если вы не предпримете дополнительные действия для их повторной отправки. После восстановления недоступного шарда вам нужно будет повторить операцию вставки вручную или через механизм повторных попыток в вашем приложении.

2. **`insert_distributed_sync = 0` (асинхронная запись):**
   - При асинхронной записи, если один из шардов недоступен, данные **будут буферизоваться локально** в каталогах на диске сервера, где был выполнен запрос на вставку.
   - Как только недоступный шард снова станет доступен, ClickHouse автоматически отправит буферизованные данные на него.
   - **Данные не потеряются**, так как они временно сохраняются на локальном сервере и будут отправлены на шард после его восстановления.

   Буферизация происходит в локальном каталоге ClickHouse, обычно по пути:
   ```
   /var/lib/clickhouse/data/default/distributed_table/shard_name/
   ```
   При использовании асинхронной вставки важно следить за размером буферизуемых данных, так как они будут храниться на диске до тех пор, пока шард не восстановится.


# **Изменение кол-ва шардов**
В ClickHouse шардирование осуществляется на уровне распределённых таблиц и серверов, что позволяет распределять данные по различным узлам кластера. Для добавления нового шарда или удаления существующего нужно учитывать несколько аспектов, таких как топология кластера, стратегия шардирования и поведение распределённых таблиц.

## **Добавление нового шарда**
Чтобы добавить новый шард в ClickHouse:

### 1. **Добавление нового узла:**
Запустите новый ClickHouse сервер и убедитесь, что его конфигурация соответствует остальным узлам в кластере (например, у него установлен `keeper` для синхронизации данных).
### 2. **Обновление конфигурации кластера:**
#### 2.1 Добавьте новый шард в конфигурацию ClickHouse (`config.xml`)

На всех узлах кластера, в секции `<remote_servers>`. Например:

     ```xml
     <remote_servers>
       <company_cluster>
         <shard>
              <replica>
                  <host>clickhouse04</host>
                  <port>9000</port>
              </replica>
          </shard>
          <!-- Новый шард -->
          <shard>
              <replica>
                  <host>clickhouse05</host>
                  <port>9000</port>
              </replica>
          </shard>
       </company_cluster>
     </remote_servers>
     ```
#### 2.2 Добавьте новый шард в конфигурацию Raft (`config.xml`)

    ```xml
    <keeper_server>
      <raft_configuration>
            <server>
                <id>04</id>
                <hostname>clickhouse04</hostname>
                <port>9234</port>
            </server>
            <!-- Новый шард -->
            <server>
                <id>05</id>
                <hostname>clickhouse05</hostname>
                <port>9234</port>
            </server>
      <raft_configuration>
    <keeper_server>
    ```
#### 2.3 Добавьте новый шард в конфигурацию Zookeeper (`config.xml`)

    ```xml
    <zookeeper>
        <node>
            <host>clickhouse04</host>
            <port>9181</port>
        </node>
         <node>
            <host>clickhouse05</host>
            <port>9181</port>
        </node>
    </zookeeper>
    ```
**Проверка состаяния текущего кластера**
```sql
SELECT cluster,
    shard_num,
    host_name,
    host_address
FROM system.clusters;
```
Примерный вывод:
```
   ┌─cluster─────────┬─shard_num─┬─host_name────┬─host_address──┐
1. │ company_cluster │         1 │ clickhouse01 │ 172.23.32.110 │
2. │ company_cluster │         2 │ clickhouse02 │ 172.23.32.120 │
3. │ company_cluster │         3 │ clickhouse03 │ 172.23.32.130 │
4. │ company_cluster │         4 │ clickhouse04 │ 172.23.32.140 │
   └─────────────────┴───────────┴──────────────┴───────────────┘
```

На данном этапе, видно, что кластер не применил настройки, для этого, нужно перезапустить ноды.

#### 2.4 По очереди, перезапустить все шарды, для применения нового конфига!
Снова выполняем команду

```sql
SELECT cluster,
    shard_num,
    host_name,
    host_address
FROM system.clusters;
```
Примерный вывод:
```
   ┌─cluster─────────┬─shard_num─┬─host_name────┬─host_address──┐
1. │ company_cluster │         1 │ clickhouse01 │ 172.23.32.110 │
2. │ company_cluster │         2 │ clickhouse02 │ 172.23.32.120 │
3. │ company_cluster │         3 │ clickhouse03 │ 172.23.32.130 │
4. │ company_cluster │         4 │ clickhouse04 │ 172.23.32.140 │
5. │ company_cluster │         5 │ clickhouse05 │ 172.23.32.150 │
   └─────────────────┴───────────┴──────────────┴───────────────┘
```

После добавления нового шарда, кластер будет работать как предполагается.
Но т.к. мы используем keeper необходимо новый шард добавить в кворум.
На данный момент keeper не умеет это делать автоматически. И на новой ноде, будет спамить такого рода лог

`<Information> KeeperDispatcher: Server still not initialized, will not apply configuration until initialization finished`

Ключевая настройка для добавления шарда

[enable_reconfiguration](https://clickhouse.com/docs/en/guides/sre/keeper/clickhouse-keeper#keeper-configuration-settings)

**debug**
SET allow_unrestricted_reads_from_keeper = 1;

SELECT * FROM system.zookeeper;

#### 2.5 Добавление новой ноды в keeper quorum
official documentation [LINK](https://clickhouse.com/docs/en/guides/sre/keeper/clickhouse-keeper#reconfiguration)

Keeper требует, чтобы состояние кластера было консистентным, и поэтому все изменения, такие как добавление новых нод, необходимо выполнять вручную.
Как вариант, можно написать скрипт автоматического добавления, но пока не вижу в этом смысла.

1. Подключаемся с любой ноды в кластере к киперу

docker exec -it -u root clickhouse06 clickhouse-keeper-client

2. смотрим список всех доступных нод на данный момент

```sh
/ :) get "/keeper/config"
server.11=clickhouse01:9234;participant;1
server.12=clickhouse02:9234;participant;1
server.13=clickhouse03:9234;participant;1
server.14=clickhouse04:9234;participant;1
server.15=clickhouse05:9234;participant;1
```

3. добавляем новую ноду в кворум

```sh
/ :) reconfig add "server.16=clickhouse06:9234;participant"
server.16=clickhouse06:9234;participant;1
server.11=clickhouse01:9234;participant;1
server.12=clickhouse02:9234;participant;1
server.13=clickhouse03:9234;participant;1
server.14=clickhouse04:9234;participant;1
server.15=clickhouse05:9234;participant;1
```

где
- `server.16` ID нового шарда
- `clickhouse06:9234` IP:port шарда
- `participant` роль шарда

Если настройка **enable_reconfiguration** будет отключена, мы *не сможем* внести изменения в кворум! И получим ошибку:

```
/ :) reconfig add "server.15=clickhouse05:9234;participent"
Coordination error: Unimplemented
```

### 3. **Автоматизация создания таблиц**

После поднятия, шард будет пуст. Ни базы данных, ни таблиц в нём не будет. Их нужно создать!

Чтобы автоматизировать процесс создания таблиц на новом шарде, можно использовать следующие подходы:

#### 3.1. **Скрипт на основе SQL-запросов**
Можно написать скрипт на Python или Bash, который:
1. Получает список всех таблиц на существующих шардах.
2. Автоматически создает аналогичные таблицы на новом шарде.

Пример на Python с использованием библиотеки `clickhouse-driver`:

```python
from clickhouse_driver import Client

existing_shard_host = '172.23.32.110'
new_shard_host = '172.23.32.150'
db_name = 'company_db'
# Подключение к существующему шару для получения списка таблиц
client = Client(host=existing_shard_host, user='default', password='')
client.execute(f"USE {db_name}")
tables = client.execute("SHOW TABLES")

# Подключение к новому шару
new_shard_client = Client(host=new_shard_host, user='default', password='')

# Проверка существования базы данных на новом шарде, создание если не существует
db_exists = new_shard_client.execute(f"EXISTS DATABASE {db_name}")
if not db_exists[0][0]:
    new_shard_client.execute(f"CREATE DATABASE {db_name}")

# Переключение на новую базу на новом шарде
new_shard_client.execute(f"USE {db_name}")

# Создание таблиц на новом шарде
for table in tables:
    create_query = client.execute(f"SHOW CREATE TABLE {db_name}.{table[0]}")[0][0]
    new_shard_client.execute(create_query)
print("Таблицы успешно скопированы на новый шард.")
```
#### 3.2. **Провести создание руками**
Команда `ON CLUSTER` в ClickHouse позволяет выполнять SQL-запросы на всех узлах кластера. Если вы добавили новый шард и хотите создать таблицы на нем, то нужно выполнить команду `CREATE TABLE` с параметром `ON CLUSTER`, и ClickHouse автоматически выполнит команду на всех шардах и репликах кластера, в том числе на новом.

```sql
CREATE TABLE IF NOT EXISTS company_db.events ON CLUSTER '{cluster}' (
    time DateTime,
    uid  Int64,
    type LowCardinality(String)
)
ENGINE = MergeTree()
PARTITION BY toDate(time)
ORDER BY (uid);
```
Этот запрос создаст таблицу `events` на всех узлах, включая новый шард. (аналогичным образом применяем создание и для ReplicatedMergeTree движка)

### 4. **Распределение данных:** (опционально)
Теперь можно начать перемещать или пересоздавать данные на новом шарде. Прямого метода автоматического перемещения данных нет, но можно использовать:

#### 4.1 **Обычное копирование**
`INSERT INTO ... SELECT ...` для копирования данных из старых шардов на новый.

#### 4.2. **Использование утилиты `clickhouse-copier`**
ClickHouse предоставляет утилиту `clickhouse-copier`, предназначенную для копирования данных между шардовыми узлами. Эта утилита может автоматизировать процесс миграции данных между шардами, но она требует настройки конфигурационного файла с указанием источника и назначения данных. Пример конфигурации:

При этом копируются:
1. **Все строки и столбцы таблиц:** Полный набор данных без исключений.
2. **Структура таблиц (если это требуется):** При настройке можно указать создание таблиц на целевом шарде, если их там еще нет.

#### Важные моменты, которые нужно учитывать:
1. **Таблицы на целевом шарде должны существовать:**
   - Перед запуском `clickhouse-copier` убедитесь, что на целевых шардах (куда будут копироваться данные) таблицы существуют и их структура полностью совпадает со структурой таблиц на исходных шардах.
   - Если таблиц нет, их можно создать вручную или использовать конфигурацию `create_tables="true"` в `clickhouse-copier`.

2. **Выбор таблиц для копирования:**
   - По умолчанию `clickhouse-copier` копирует все данные из указанных таблиц. Если нужно скопировать только часть данных (например, по конкретному временному интервалу или условию), это можно настроить с помощью параметров задания.

3. **Проблемы с конфигурацией:**
   - Убедитесь, что правильно настроены секции `source` (источник) и `destination` (назначение), чтобы указать копирование данных между конкретными шардами и репликами.

4. **Идёмпотентность (безопасность повторного запуска):**
   - `clickhouse-copier` гарантирует идемпотентность операций — повторное копирование не приведет к дублированию данных, если задания настроены корректно.

```xml
<job>
    <settings>
        <create_tables>true</create_tables> <!-- Создать таблицы на целевом шарде, если их нет -->
        <max_throttling_queries>2</max_throttling_queries> <!-- Параллельность запросов -->
        <max_bandwidth_for_parallel_replication>1000000000</max_bandwidth_for_parallel_replication> <!-- Ограничение по скорости копирования -->
    </settings>
    <task>
        <table>my_table</table>
        <shard>
            <replica>
                <host>source-shard-host</host>
                <port>9000</port>
            </replica>
        </shard>
        <destination>
            <shard>
                <replica>
                    <host>new-shard-host</host>
                    <port>9000</port>
                </replica>
            </shard>
        </destination>
    </task>
</job>
```

Запустите копирование:

```bash
clickhouse-copier --config config.xml
```

### Итог
Для полного добавления нового шарда в кластер и создания идентичных таблиц следует:
1. Использовать `ON CLUSTER` для создания таблиц на новом шарде.
2. При необходимости — использовать `clickhouse-copier` для миграции данных.

## **Удаление шарда**
Для удаления шарда из кластера:

### 1. **Измените конфигурацию:**
Удалите соответствующий шард из конфигурации `config.xml` в секции `<remote_servers>`, `raft_configuration`, `zookeeper` на всех узлах.
### 2. **Удалите или перенесите данные:**
Если данные на удаляемом шарде необходимы, переместите их на другие шарды с помощью SQL-запросов.
### 3. **Удалите шард из распределённой таблицы:**

   ```sql
   ALTER TABLE <distributed_table_name> DELETE SHARD 'shard_to_remove';
   ```

### 4. **Перезапуск ClickHouse:**
Перезапустите все узлы ClickHouse, чтобы изменения вступили в силу.

### 5. **Удаление шарда из keeper quorum**

```sh
/ :) reconfig remove "15,16"
server.11=clickhouse01:9234;participant;1
server.12=clickhouse02:9234;participant;1
server.13=clickhouse03:9234;participant;1
server.14=clickhouse04:9234;participant;1
```

# **Поведение при нехватке места на одном из шардов**
Если на одном из шардов заканчивается место, ClickHouse будет продолжать записывать данные на другие доступные шарды. Однако на шарде, на котором нет свободного места, любые операции записи будут завершаться ошибками. Поведение системы зависит от стратегии шардирования:

- **Если используется взвешенное шардирование (`weight`):** Данные будут записываться на те шарды, у которых еще есть место. Задача распределения данных между шардовыми узлами будет выполнять балансировку, исходя из веса.
- **Если нет взвешивания:** Запись на полностью заполненный шард будет завершаться ошибкой, но система продолжит функционировать на других шардах.

Для мониторинга свободного места на шардах можно использовать такие SQL-запросы:

```sql
SELECT
    shard,
    free_space,
    total_space,
    free_space / total_space * 100 AS free_space_percent
FROM system.disks
```

### 4. **Рекомендации по управлению шардами**
- Следите за распределением данных, чтобы избежать дисбаланса между шардами.
- Внедрите мониторинг свободного места и состояния шардов для предотвращения ситуаций с нехваткой ресурсов.
- Если возможен сценарий с нехваткой места, стоит использовать стратегию репликации, чтобы данные автоматически распределялись между репликами, что снижает вероятность отказа при записи.
