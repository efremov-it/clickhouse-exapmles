Рассчитать нагрузку на сеть и файловую систему для многонодового ClickHouse кластера с репликами можно, используя параметры, такие как объем записываемых и читаемых данных, частота репликации, а также сетевые ограничения между нодами.

### Основные факторы для оценки нагрузки

1. **Нагрузка на сеть**:
   - Объем передаваемых данных между нодами для репликации.
   - Количество запросов на чтение и запись, если репликация настраивается синхронно или асинхронно.
   - Тип репликации: частота и объем данных, передаваемых через сеть.

2. **Нагрузка на файловую систему**:
   - Частота и объем операций ввода-вывода (I/O) для каждой реплики.
   - Количество доступных IOPS (операций ввода-вывода в секунду) для PVC.
   - Уровень параллельных запросов на чтение/запись, что определяет, насколько интенсивно будет использоваться файловая система.

### Примерная оценка нагрузки на сеть и файловую систему

Для простоты предположим следующие базовые параметры:
- **Чтение/запись данных**: 1 ГБ данных в час на реплику.
- **Количество операций**: Каждая запись и чтение генерирует примерно 1 000 операций в час на одну реплику.
- **Количество реплик и нод**: Используем 3 ноды с 3 шардами (с репликами), как в примере выше.

#### Нагрузка на сеть

1. **Репликация данных**:
   - При каждом изменении данных для шардов происходит репликация на другие ноды.
   - С учетом объема в 1 ГБ на реплику, репликация будет генерировать примерно **2 ГБ передачи данных в час на ноду**.
   - Итоговая нагрузка на сеть для 3 нод составит около **6 ГБ/час** при синхронной репликации.

2. **Сетевые задержки**:
   - Репликация требует стабильного сетевого соединения. Если доступная пропускная способность сети ниже 100 Мбит/с, могут возникать задержки, особенно при увеличении объема данных.
   - Сетевой интерфейс между нодами должен поддерживать минимум 100 Мбит/с для таких объемов, но рекомендуется 1 Гбит/с для комфортной репликации без задержек.

#### Нагрузка на файловую систему

1. **Записи данных**:
   - При 1 ГБ записи на реплику и 3 нодах с 3 репликами общая нагрузка на файловую систему составит **около 9 ГБ в час**.
   - Для PVC с 1000 IOPS, объем в 1 ГБ данных приведет к дополнительной нагрузке около 1 000 IOPS на запись для каждой ноды.

2. **Чтения данных**:
   - Если каждую реплику запрашивают с частотой 1000 операций в час, нагрузка на файловую систему для чтений составит **около 3 000 IOPS на ноду** (по 1000 IOPS на каждую реплику).
   - Это предполагает наличие быстрого хранилища, способного выдерживать как нагрузку на запись, так и на чтение.

#### Общая нагрузка

- **Сеть**: Около 6 ГБ/час передачи данных (при синхронной репликации) и минимальная скорость сети 100 Мбит/с, желательно 1 Гбит/с.
- **Файловая система**: Около 9 ГБ/час записи и 3 000 IOPS для чтений.

### Рекомендации для оптимизации

1. **Увеличение пропускной способности сети**:
   - Если объем данных и скорость репликации значительно увеличиваются, переход на 10 Гбит/с сеть позволит уменьшить задержки и улучшить производительность.
  
2. **Оптимизация файловой системы**:
   - Использование SSD-хранилища для PVC с высокой скоростью IOPS и низкими задержками на запись и чтение.
   - Выделение отдельных дисков для логов и данных для ClickHouse, чтобы снизить конкуренцию за ресурсы ввода-вывода.

3. **Настройка репликации**:
   - Настройка асинхронной репликации поможет уменьшить нагрузку на сеть и уменьшить задержки.
   - Ограничение скорости репликации при высокой нагрузке может уменьшить пиковую нагрузку на сеть и диски.

Эти рекомендации и оценки помогут распределить нагрузку и обеспечить отказоустойчивость для кластера, минимизируя при этом задержки и обеспечивая необходимую производительность.

<internal_replication>true</internal_replication>
<prefer_localhost_replica>true</prefer_localhost_replica>
<insert_distributed_sync>0</insert_distributed_sync>
<skip_unavailable_shards>1</skip_unavailable_shards>

С учетом добавленных настроек, нагрузка на сеть и на диски в ClickHouse изменится следующим образом:

### Влияние параметров

1. **`<internal_replication>true</internal_replication>`**
   - Включение внутренней репликации (`internal_replication`) указывает ClickHouse, что он должен автоматически управлять репликацией на уровне шарда. Это означает, что каждый узел будет управлять своими локальными репликами без необходимости отправки всех данных через сеть для межнодовой репликации.
   - **Сетевые ресурсы**: Снижается нагрузка на сеть, поскольку репликация в основном происходит на уровне локальных дисков.
   - **Дисковые ресурсы**: Увеличивается нагрузка на файловую систему, так как данные записываются на диски всех реплик на локальных нодах. Это требует больших объемов дискового пространства и IOPS.

2. **`<prefer_localhost_replica>true</prefer_localhost_replica>`**
   - Настройка `prefer_localhost_replica` на `true` указывает ClickHouse использовать локальную реплику для запросов, если она доступна.
   - **Сетевые ресурсы**: Нагрузка на сеть при чтении уменьшается, так как основная часть запросов будет обслуживаться локально (минимизируется межнодовая передача данных).
   - **Дисковые ресурсы**: Увеличивается нагрузка на локальные диски, так как ноды в большей степени зависят от локальной реплики. Это приводит к увеличению IOPS на локальные диски при обработке запросов.

3. **`<insert_distributed_sync>0</insert_distributed_sync>`**
   - Установка `insert_distributed_sync` на `0` означает асинхронное распределение данных. В этом режиме ClickHouse не будет ждать, пока данные реплицируются на все ноды, что снижает задержки при вставках.
   - **Сетевые ресурсы**: Нагрузка на сеть становится более плавной, так как асинхронная вставка уменьшает пиковые нагрузки на сеть, позволяя распределять трафик.
   - **Дисковые ресурсы**: Периодически увеличивается нагрузка на файловую систему, так как вставка данных происходит без ожидания их полной репликации. Диски могут подвергаться кратковременным пиковым нагрузкам, когда данные догоняются репликацией.

4. **`<skip_unavailable_shards>1</skip_unavailable_shards>`**
   - Эта настройка позволяет ClickHouse пропускать недоступные шарды при запросах.
   - **Сетевые ресурсы**: Уменьшается нагрузка на сеть, так как запросы не будут пытаться обращаться к недоступным шардом. Снижается вероятность переполнения сети из-за недоступности части нод.
   - **Дисковые ресурсы**: Временное увеличение нагрузки на оставшиеся доступные шарды, так как они временно обрабатывают больше запросов, пока недоступные шарды не восстановятся.

### Изменение нагрузки на сеть и диски с учетом настроек

#### Сетевые ресурсы

С учетом всех вышеперечисленных настроек, нагрузка на сеть уменьшится:

- **Снижение нагрузки при вставках**: Асинхронная вставка (`insert_distributed_sync=0`) позволяет избежать пиковых нагрузок на сеть при записи, так как данные распределяются асинхронно.
- **Локальные запросы**: Параметр `prefer_localhost_replica` снижает количество межнодовых сетевых операций при чтении, поскольку запросы будут обслуживаться локально, если доступна локальная реплика.
- **Пропуск недоступных шардов**: `skip_unavailable_shards=1` помогает уменьшить нагрузку на сеть, так как запросы не будут пытаться обращаться к недоступным шардом, исключая повторные попытки сетевых запросов.

#### Дисковые ресурсы

С учетом этих настроек нагрузка на файловую систему возрастет:

- **Увеличение нагрузки на локальные диски**: Локальные реплики (`prefer_localhost_replica=true`) будут использоваться чаще, что приводит к увеличению локальной нагрузки на чтение/запись и повышенным IOPS на дисках каждой ноды.
- **Пиковая нагрузка на диски при репликации**: Хотя `insert_distributed_sync=0` распределяет нагрузку на сеть асинхронно, данные всё равно записываются на диски, что создает повышенную нагрузку в моменты догоняющей репликации.
- **Пропуск недоступных шардов**: Если один или несколько шардов временно недоступны, оставшиеся шарды будут принимать больше запросов, что также увеличивает локальную нагрузку на их диски.

### Подведем итоги

- **Сеть**: Нагрузка на сеть уменьшится за счёт локального обслуживания запросов и асинхронной вставки.
- **Файловая система**: Нагрузка на диски увеличится из-за частого использования локальных реплик и пиковых вставок, которые будут догоняться в асинхронном режиме.

Эти настройки делают кластер более устойчивым к сетевым перегрузкам и недоступности шардов, при этом немного увеличивая нагрузку на локальные диски.
----------------------------------------

У нас 3 шарда, у каждого по 3 реплики, при этом, они находятся на 3-х различных нодах, значит запись данных 1ГБ будет рапределена по 3-м шардам и обязательно записана только на одну реплику каждого из них. При этом, чтение, тоже будет проходить сразу с 3-х машин (шардов). 
Учти эти данные и обнови результаты расчетов

Нода 1 содержит три реплики (shard0.replica0, shard1.replica0, shard2.replica0).
Нода 2 содержит три реплики (shard0.replica1, shard1.replica1, shard2.replica1).
Нода 3 содержит три реплики (shard0.replica2, shard1.replica2, shard2.replica2)

----------------------------------------
При наличии 3 шардов с 3 репликами, распределенных по 3 нодам, а также с учетом новых настроек, перерасчет сетевой и дисковой нагрузки будет следующим:

### Условия для расчетов

1. **Запись данных**:
   - Запись 1 ГБ данных делится между тремя шардами, каждый из которых получает около **333 МБ данных**.
   - С учетом настройки `internal_replication=true`, данные записываются на одну реплику каждого шарда и затем синхронизируются асинхронно с другими репликами.

2. **Чтение данных**:
   - Настройка `prefer_localhost_replica=true` распределяет запросы по локальным репликам, если они доступны, что минимизирует межнодовый трафик и уменьшает сетевые задержки.
   - Поскольку запросы будут распределяться по трем нодам, каждая из которых обрабатывает свой локальный шард, это снижает нагрузку на каждую отдельную ноду.

### Обновленные расчеты нагрузки на сеть и файловую систему

#### Нагрузка на сеть

С учетом асинхронной репликации и локальных реплик, сетевой трафик для записи и чтения перераспределяется следующим образом:

1. **Запись данных (insert_distributed_sync=0)**:
   - При записи 1 ГБ данных каждый шард получает по 333 МБ данных, что первоначально записывается на одну реплику. Асинхронная репликация передает данные на другие две реплики по мере их обновления.
   - С учетом асинхронной передачи данных на реплики, сетевой трафик для записи составляет около **2 ГБ/час** при вставке 1 ГБ/час.

2. **Чтение данных (prefer_localhost_replica=true)**:
   - Локальные реплики обрабатывают около 70% запросов на чтение, распределяя трафик по нодам, что минимизирует межнодовые операции.
   - Оставшиеся 30% запросов при необходимости обращаются к удаленным репликам, создавая сетевой трафик для чтений.
   - При чтении 1 ГБ данных в час ожидается нагрузка в **примерно 600 МБ/час на межнодовую сеть** (ранее было 3 ГБ/час), так как основная часть данных читается локально.

**Итоговая нагрузка на сеть**:
- **Запись**: 2 ГБ/час на межнодовую сеть.
- **Чтение**: 600 МБ/час на межнодовую сеть.

Минимальная пропускная способность сети может составлять **10-20 Мбит/с**, с рекомендуемой **50 Мбит/с** для поддержания стабильного взаимодействия между нодами.

#### Нагрузка на файловую систему

За счет использования локальных реплик и асинхронной репликации нагрузка на диски распределяется следующим образом:

1. **Запись данных**:
   - Для каждого шарда, реплика которого получает 333 МБ данных, данные записываются локально и затем асинхронно распространяются на другие две реплики.
   - Это приводит к **общему объему записи около 1 ГБ/час на каждую ноду**.

2. **Чтение данных**:
   - Локальные реплики обрабатывают большинство запросов, распределяя нагрузку на чтение между нодами.
   - IOPS на каждую ноду для чтения при этом уменьшится, так как запросы распределяются равномерно.
   - **Ожидается около 2 100 IOPS для каждой ноды**, так как запросы к данным читаются с локальных реплик и распределяются между 3 нодами.

**Итоговая нагрузка на файловую систему**:
- **Запись**: около 1 ГБ/час на каждую ноду.
- **Чтение**: около 2 100 IOPS на чтение для каждой ноды.

### Вывод

С учетом новой конфигурации, нагрузка на сеть и файловую систему оптимизируется следующим образом:

- **Нагрузка на сеть** снижается за счет использования локальных реплик, составив примерно **2.6 ГБ/час** (2 ГБ на запись и 600 МБ на чтение), что позволяет уменьшить требования к пропускной способности сети.
- **Нагрузка на диски** распределяется между нодами, достигая около **1 ГБ/час записи на каждую ноду** и **2 100 IOPS на чтение**, что соответствует балансированному использованию ресурсов и высокой отказоустойчивости.

------------------------------------------------
------------------------------------------------

При использовании трех шардов с двумя репликами для каждого шарда и при наличии трех нод, равномерное распределение подов по нодам может выглядеть так:

### Распределение по нодам:

#### Нода 1:
- `shard0.replica0`
- `shard1.replica1`

#### Нода 2:
- `shard1.replica0`
- `shard2.replica1`

#### Нода 3:
- `shard0.replica1`
- `shard2.replica0`

### Обоснование
В этом варианте:

- Каждая нода обслуживает два различных шарда.
- Реплики одного шарда не находятся на одной ноде, что обеспечивает отказоустойчивость.
- Нагрузка распределяется равномерно, поскольку каждая нода имеет по два пода (реплики) для различных шардов.

### PVC требования
Каждая реплика в этой конфигурации требует **PVC с типом `ReadWriteOnce`** для хранения данных, при этом:

- **Нода 1**: имеет два PVC — один для `shard0.replica0` и один для `shard1.replica1`.
- **Нода 2**: имеет два PVC — один для `shard1.replica0` и один для `shard2.replica1`.
- **Нода 3**: имеет два PVC — один для `shard0.replica1` и один для `shard2.replica0`.

Это распределение обеспечивает равномерную нагрузку на каждую ноду и повышает отказоустойчивость системы.



------------------------------------------------

Давайте пересчитаем нагрузку на сеть и файловую систему с учетом нового распределения — 3 шарда с 2 репликами для каждого, равномерно распределенными по 3 нодам. 

### Основные изменения

1. **Запись данных**: 
   - 1 ГБ данных теперь также распределяется по 3 шардовым репликам (по 333 МБ на каждый шард), но каждая реплика имеет только 1 дополнительную копию.
   - Данные записываются на одну реплику каждого шарда, а затем асинхронно реплицируются на вторую.

2. **Чтение данных**:
   - Чтения преимущественно обращаются к локальным репликам (благодаря `prefer_localhost_replica=true`), что снижает межнодовый трафик.
   - Чтения делятся между 2 репликами каждого шарда.

### Обновленные расчеты для нагрузки на сеть и файловую систему

#### Нагрузка на сеть

С учетом асинхронной репликации и уменьшения количества реплик:

1. **Запись данных**:
   - Данные для каждого шарда (333 МБ) записываются на основную реплику и асинхронно передаются на вторую реплику, генерируя трафик только для одной копии.
   - При вставке 1 ГБ данных в час сетевой трафик составит **около 1 ГБ/час** (ранее было 2 ГБ/час, так как теперь только одна реплика копирует данные).

2. **Чтение данных**:
   - Локальные реплики обрабатывают до 70% запросов на чтение, а 30% обращаются к удаленным репликам.
   - С учетом уменьшенного количества удаленных чтений, ожидается **около 300 МБ/час** сетевого трафика (ранее 600 МБ/час).

**Итоговая нагрузка на сеть**:
- **Запись**: 1 ГБ/час на межнодовую сеть.
- **Чтение**: 300 МБ/час на межнодовую сеть.

Минимальная пропускная способность сети может составлять **10 Мбит/с**, с рекомендуемой **50 Мбит/с** для стабильной работы.

#### Нагрузка на файловую систему

За счет уменьшения количества реплик нагрузка на диски также перераспределяется:

1. **Запись данных**:
   - На каждую основную реплику каждого шарда записывается 333 МБ данных, которые затем копируются на одну дополнительную реплику.
   - **Итоговая запись на каждую ноду** составит примерно **1 ГБ/час** (как и ранее, так как данные распределяются между основными и дополнительными репликами равномерно).

2. **Чтение данных**:
   - Чтения делятся между двумя репликами для каждого шарда, что снижает требуемые IOPS для чтения.
   - **Ожидается примерно 2 000 IOPS на чтение для каждой ноды** (немного снижено по сравнению с 2 100 IOPS).

**Итоговая нагрузка на файловую систему**:
- **Запись**: около 1 ГБ/час на каждую ноду.
- **Чтение**: около 2 000 IOPS на чтение для каждой ноды.

### Итог

С учетом уменьшенного количества реплик и асинхронной репликации нагрузка на сеть и диски будет ниже:

- **Нагрузка на сеть**: примерно **1.3 ГБ/час** (1 ГБ на запись и 300 МБ на чтение), что требует минимальной пропускной способности сети 10 Мбит/с.
- **Нагрузка на файловую систему**: **1 ГБ/час записи** и **2 000 IOPS на чтение для каждой ноды**, что обеспечивает баланс нагрузки и отказоустойчивость системы.





На основе требований к отказоустойчивости и производительности в условиях трехноды кластера, наиболее подходящим вариантом будет конфигурация с **3 шардов и 2 реплик для каждого**. Давайте разберем, почему именно эта конфигурация предпочтительнее, и рассмотрим альтернативные подходы:

### 1. Конфигурация с 3 шардов и 2 репликами

**Преимущества:**
- **Отказоустойчивость:** Система может выдерживать отказ любой одной ноды без потери данных и доступа к ним. При падении одной ноды все данные все еще будут доступны через оставшиеся реплики.
- **Балансировка нагрузки:** Шардирование позволяет распределить запросы на чтение и запись между несколькими репликами, что повышает пропускную способность и снижает задержки.
- **Экономия ресурсов сети:** За счет наличия одной реплики для каждого шарда на каждой из нод уменьшается межнодовая передача данных (асинхронная репликация передает данные только на одну копию).
- **Эффективное чтение:** Настройка `prefer_localhost_replica=true` позволяет снизить нагрузку на сеть и повысить производительность чтения, обращаясь к локальным репликам на каждой ноде.

### Почему не стоит использовать 3 шарда с 3 репликами?

Конфигурация с 3 шардов и 3 репликами, где каждый шард имеет по одной реплике на каждой ноде, обеспечивает максимальную отказоустойчивость, так как каждый шард полностью дублирован на всех нодах. Однако такой подход существенно увеличивает:
- **Нагрузку на сеть:** при увеличении количества реплик возрастает объем межнодовой передачи данных, так как каждая вставка данных требует синхронизации с двумя дополнительными репликами.
- **Использование дискового пространства:** каждая нода должна хранить полный объем данных, что менее эффективно с точки зрения ресурсов диска, чем при 2 репликах.

### Альтернативный вариант: 2 шарда с 3 репликами

При конфигурации с 2 шардов и 3 репликами можно снизить потребности в шардировании, так как объем данных будет распределен между двумя шардов, а каждая нода будет иметь реплику каждого шарда.

**Преимущества:**
- Высокая отказоустойчивость — каждый шард представлен на каждой ноде, и каждая нода может обрабатывать запросы.
- Снижение задержек за счет меньшего количества шардов, что упрощает управление репликами.

**Недостатки:**
- Более высокая нагрузка на диски, так как на каждой ноде хранится полный объем данных двух шардов.
- Снижение пропускной способности при масштабировании, так как меньшее количество шардов ограничивает параллельную обработку запросов.

### Итоговый выбор

Конфигурация с **3 шардов и 2 репликами** является наиболее сбалансированным вариантом для трехноды кластера, обеспечивая:

1. **Отказоустойчивость** (выдерживает потерю одной ноды).
2. **Балансировку нагрузки** и лучшую производительность за счет шардирования.
3. **Оптимальное использование сети и дискового пространства** с учетом асинхронной репликации.

Таким образом, для текущей архитектуры с трехноды кластером в Kubernetes лучшим выбором будет **конфигурация с 3 шардов и 2 репликами**.

