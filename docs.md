# Базовые положения основных сущностей
- Реплика
Это простая копия данных. ClickHouse всегда имеет хотя бы одну копию ваших данных, поэтому минимальное количество реплик — одна. Это важная деталь: оригинал данных не считается репликой, однако в коде и документации ClickHouse используется система, при которой все инстансы — это реплики. 

- Шард
Подмножество данных, в котором шардированные данные разбиваются на непересекающиеся наборы. Допустим, у нас есть таблица с большим количеством логов, которая физически не помещается на одном сервере. Мы можем разбить таблицу на 4 части и разместить на 4 разных серверах. Это позволяет не только уменьшить объём данных на одном инстансе, но и увеличить пропускную способность системы и количество операций ввода/вывода за счёт того, что операции выполняются на всех хостах.

ClickHouse всегда имеет хотя бы один шард для ваших данных. Поэтому, если вы не распределяете данные по нескольким серверам, ваши данные будут храниться в одном шарде. Целевой сервер определяется ключом партицирования и определяется при создании распределенной таблицы. 

- Метадата
Информация, которую описывают другие данные и представляют собой структурированные справочники, что помогают сортировать и идентифицировать атрибуты описываемой ими информации.

# Репликация
### Зачем она нужна?

- Обеспечение высокой доступности
- Масштабирование нагрузки
  Есть несколько способов обеспечить масштабирование
  1. Через шардирование
    Недостаток, отказоустойчивость
  2. Через репликацию
    Не проверял
  3. Через шардирование+репликация (для каждого шарда, создается своя репликация)
    Недостаток: нужно иметь в 2 раза больше машин. (2 на шарды 2 на реплики шардов)
    Этот недостаток нивелируется при поднятии контейнеров в kubernetes cluster.
    Как вариант. У нас есть 2 машины в кластере, выделенные именно под clickhouse.
    На каждой ихз них у нас поднято по 2 контейнера
    1. Контейнер с шардом `1`(clickhouse01) контейнер с репликой шарда `2` (clickhouse04)
    2. Контейнер с шардом `2` (clickhouse03) контейнер с репликой шарда `1` (clickhouse02)
    В этом случае, при потере одной из машин, у нас остается в работе как минимум 1 рабочий шард и реплика потерянного шарда.
    Но при стабильной работе кластера, нагрузка на чтение и запись, распределяется равномерно между двумя шардами в кластере.
- Упрощение процесса миграции и обновления

### Взаимодействие компонентов
Для хранения и координации действий репликации в ClickHouse требуется распределённое хранилище данных, которое может гарантировать согласованность состояния. Для этого ClickHouse использует один из двух вариантов: Zookeeper или ClickHouse Keeper.

#### Keeper
Keeper не хранит в себе данные таблиц, но содержит метаданные, которые говорят, какие данные должны хранится в каждой реплике. Например, если вы вставляете 10 строк, то ClickHouse создаст парт и ссылка на этот парт будет хранится в Keeper. Все инстансы ClickHouse будут знать, что парт есть в кластере и необходимо найти его в локальной файловой системе. А если его нет локально, то инстанс должен обратиться к другим инстансам, чтобы скопировать его. Корректность метаданных и доступность самого Keeper критически важны для корректной работы репликации: если вы потеряете свой Keeper, то все реплицируемые таблицы уйдут в read-only режим до возвращения Keeper в рабочее состояние. Сейчас внутри ClickHouse есть функция SystemRestoreReplica, которая позволяет восстановить репликация из локальных данных, однако этот вариант стоит использовать только в крайнем случае. Помимо хранения метаданных, ClickHouse также использует Keeper для координации кластера.

!TODO ### Нужно уточнить информацию относительно использования keeper на каждой отдельной ноде. Если брать во внимание наш способ разворачивания кластера, то исмпользование keeper на первый взгляд предпочтительней. 
ClickHouse Keeper можно поддерживать в двух разных установках: в качестве обособленной инсталляции на отдельных серверах или как часть ClickHouse в виде Process Keeper — части бинарного файла ClickHouse clickhouse-server. Несмотря на то, что второй вариант кажется проще и дешевле (так как по сути Keeper уже встроен в ClickHouse), мы настоятельно рекомендуем использовать ClickHouse Keeper как отдельную инсталляцию, так как это повышает отказоустойчивость системы в целом.

#### ZooKeeper

Внутри Zookeeper все данные о кластере будут хранится в директории /clickhouse и в ней будет находятся следующее:

ddl — все DDL запросы, которые необходимо реплицировать и применять к репликам.
metadata — хранит часть информации о метаданных реплицируемой таблицы, например первичный ключ, ключ раздела и т. д;
columns — хранит информацию о полях реплицируемой таблицы;
block_numbers — хранит все разделы таблицы;
leader_election — используется для выбора мастера между репликами. При запросе сначала будет выбрана ведущая реплика, а затем на ее основе будет синхронизирована другая реплика. Несколько реплик могут быть лидерами одновременно;
log — каталог, служащий очередью задач для хранения операций над репликой таблицы;
blocks — хранит хэш-информацию блоков данных, записанных в таблицу за определенный период времени для дедупликации. Формат следующих дочерних узлов — partition_hash_hash;
mutation — каталог, служащий очередью задач для хранения операций мутации над реплицированной таблицей;
replicas/node_hostname — хранит информацию о репликах:
is_active — флаг, указывающий, активен ли инстанс или нет, если на сервере произошел сбой, узел не будет существовать, и будет добавлен заново после восстановления;
mutation_pointer — хранит следующую задачу в очереди мутаций, которую следует выполнить;
log_pointer — хранит следующую задачу в очереди журналов, которую следует выполнить;
is_lost — флаг, указывающий, устарела ли реплика, на основании того, обновлён ли указатель log_pointer, при этом 0 — нормально, -1 — устарела, а 1 — находится в процессе восстановления;
metadata — хранит информацию о метаданных таблицы, как и metadata выше;
columns — хранит информацию о столбцах таблицы;
parts — хранит информацию о партах таблицы, каждый блок информации содержит контрольные суммы и информацию о столбцах;
queue — временная очередь обработки


## Создание сущностей

В рамках MergeTree вы вставляете парты, а ClickHouse будет производить их вставку и слияние с другими партами в фоновом режиме, количество потоков для выполнения фоновых задач можно задать с помощью настройки background_schedule_pool_size.

DDL и TRUNCATE операции также координируются, если применены с ON CLUSTER. Таким образом, почти все операции вставки или изменения таблиц будут реплицироваться, кроме следующих:

CREATE table;
DROP table;
DETACH table;
ATTACH table;
ALTER TABLE FREEZE;
ALTER TABLE MOVE TO DISK;
ALTER TABLE FETCH;

Указанные выше операции являются локальными для инстанса и их необходимо выполнять на всех узлах кластера отдельно.

## Переход
Если в процессе эксплуатации появляется необходимость перейти с нереплицируемой MergeTree таблицы на её Replicated-версию, то есть два основных варианта:

1) Ручной: создать реплицируемую таблицу с таким же движком и структурой, после чего присоединить партиции через ATTACH одну за одной. Этот вариант работает на всех версиях ClickHouse.  

2) Автоматический: можно добавить флаг репликации в каталог таблицы и ClickHouse автоматически преобразует таблицу в реплицируемую. Эта функция была введена в версии 24.2.

На основе все вышеописанного можно заключить, что репликация в ClickHouse — это надёжная конструкция, не обладающая точками отказа. И в целом так оно и есть: по нашему опыту репликация в ClickHouse действительно надёжна и почти никогда не ломается. Почти…


# Важно обратить внимание на:

Keeper должен находится на отдельных серверах, желательно на SSD дисках 3,5 или 7 узлов;

Не стоит очищать Keeper вручную;

Делать бэкап информации из Keeper;

Систематически обновлять версию ClickHouse для возможности использовать все актуальные инструменты;


### Проблемы
1. При удалении контейнера, данные автоматически не реплицируются (имею ввиду что не создается база данных и таблицы)

Данные автоматически реплицируются если разница между данными не велика 

Когда реплика была полностью потеряна (в данном примере это clickhouse01). Или данные с неё были полностью удалены, *нужно создать базу и таблицы руками*.
После создания базы, воссоздание потерянных таблиц может быть проблематично, это связанно с ошибкой 
`Code: 253. DB::Exception: Replica /clickhouse/tables/company_cluster/01/events/replicas/clickhouse01 already exists `
в которой говорится что (потерянная)реплика уже существует, хотя по факту её нет, так происходит из-за того, что на второй реплике (рабочей), сохранилось упоминание о ней.
Для устранения этой ошибки, нужно зайти на (рабочую) реплику и выполнить:
1. Проверить точно ли текущая реплика является клоном потерянной
`SELECT * FROM system.replicas WHERE table = 'events' FORMAT Vertical;`

Премерный вывод
```sh
database:                    company_db
table:                       events
engine:                      ReplicatedMergeTree
is_leader:                   1
can_become_leader:           1
is_readonly:                 0
is_session_expired:          0
future_parts:                0
parts_to_check:              0
zookeeper_path:              /clickhouse/tables/company_cluster/01/events
replica_name:                clickhouse01
replica_path:                /clickhouse/tables/company_cluster/01/events/replicas/clickhouse01
columns_version:             -1
queue_size:                  0
inserts_in_queue:            0
merges_in_queue:             0
part_mutations_in_queue:     0
queue_oldest_time:           1970-01-01 03:00:00
inserts_oldest_time:         1970-01-01 03:00:00
merges_oldest_time:          1970-01-01 03:00:00
part_mutations_oldest_time:  1970-01-01 03:00:00
oldest_part_to_get:          
oldest_part_to_merge_to:     
oldest_part_to_mutate_to:    
log_max_index:               1
log_pointer:                 2
last_queue_update:           1970-01-01 03:00:00
absolute_delay:              0
total_replicas:              2
active_replicas:             2
last_queue_update_exception: 
zookeeper_exception:         
replica_is_active:           {'clickhouse01':1,'clickhouse02':1}
```

2. Удаление метаданных реплики
`SYSTEM DROP REPLICA 'clickhouse01';`

После выполнения этой команды, снова проверяем состояние реплики пердыдущей командой
```sh
zookeeper_exception:         
replica_is_active:           {'clickhouse02':1}
```
3. Создать утерянную таблицу, аналогичным образом как была создана таблица на реплике.
Если нет рядом информации как была создана таблица, можем посмотреть на реплике, используя команду
`SHOW CREATE TABLE events;`

На основе полученного результата, создаем таблицу.
Проводим эту операцию с каждой таблицей.

*После пересоздания таблицы, данные автоматически реплицируются в неё*

4. Провести данную операцию для каждой таблицы.
