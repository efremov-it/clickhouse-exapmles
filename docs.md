# Базовые положения основных сущностей
- Реплика
Это простая копия данных. ClickHouse всегда имеет хотя бы одну копию ваших данных, поэтому минимальное количество реплик — одна. Это важная деталь: оригинал данных не считается репликой, однако в коде и документации ClickHouse используется система, при которой все инстансы — это реплики. 

- Шард
Подмножество данных, в котором шардированные данные разбиваются на непересекающиеся наборы. Допустим, у нас есть таблица с большим количеством логов, которая физически не помещается на одном сервере. Мы можем разбить таблицу на 4 части и разместить на 4 разных серверах. Это позволяет не только уменьшить объём данных на одном инстансе, но и увеличить пропускную способность системы и количество операций ввода/вывода за счёт того, что операции выполняются на всех хостах.

ClickHouse всегда имеет хотя бы один шард для ваших данных. Поэтому, если вы не распределяете данные по нескольким серверам, ваши данные будут храниться в одном шарде. Целевой сервер определяется ключом партицирования и определяется при создании распределенной таблицы. 

- Метадата
Информация, которую описывают другие данные и представляют собой структурированные справочники, что помогают сортировать и идентифицировать атрибуты описываемой ими информации.

# Репликация
### Зачем она нужна?

- Обеспечение высокой доступности
- Масштабирование нагрузки
  Есть несколько способов обеспечить масштабирование
  1. Через шардирование
    Недостаток, отказоустойчивость
  2. Через репликацию
    Не проверял
  3. Через шардирование+репликация (для каждого шарда, создается своя репликация)
    Недостаток: нужно иметь в 2 раза больше машин. (2 на шарды 2 на реплики шардов)
    Этот недостаток нивелируется при поднятии контейнеров в kubernetes cluster.
    Как вариант. У нас есть 2 машины в кластере, выделенные именно под clickhouse.
    На каждой ихз них у нас поднято по 2 контейнера
    1. Контейнер с шардом `1`(clickhouse01) контейнер с репликой шарда `2` (clickhouse04)
    2. Контейнер с шардом `2` (clickhouse03) контейнер с репликой шарда `1` (clickhouse02)
    В этом случае, при потере одной из машин, у нас остается в работе как минимум 1 рабочий шард и реплика потерянного шарда.
    Но при стабильной работе кластера, нагрузка на чтение и запись, распределяется равномерно между двумя шардами в кластере.
- Упрощение процесса миграции и обновления

### Взаимодействие компонентов
Для хранения и координации действий репликации в ClickHouse требуется распределённое хранилище данных, которое может гарантировать согласованность состояния. Для этого ClickHouse использует один из двух вариантов: Zookeeper или ClickHouse Keeper.

#### Keeper
Keeper не хранит в себе данные таблиц, но содержит метаданные, которые говорят, какие данные должны хранится в каждой реплике. Например, если вы вставляете 10 строк, то ClickHouse создаст парт и ссылка на этот парт будет хранится в Keeper. Все инстансы ClickHouse будут знать, что парт есть в кластере и необходимо найти его в локальной файловой системе. А если его нет локально, то инстанс должен обратиться к другим инстансам, чтобы скопировать его. Корректность метаданных и доступность самого Keeper критически важны для корректной работы репликации: если вы потеряете свой Keeper, то все реплицируемые таблицы уйдут в read-only режим до возвращения Keeper в рабочее состояние. Сейчас внутри ClickHouse есть функция SystemRestoreReplica, которая позволяет восстановить репликация из локальных данных, однако этот вариант стоит использовать только в крайнем случае. Помимо хранения метаданных, ClickHouse также использует Keeper для координации кластера.

!TODO ### Нужно уточнить информацию относительно использования keeper на каждой отдельной ноде. Если брать во внимание наш способ разворачивания кластера, то исмпользование keeper на первый взгляд предпочтительней. 
ClickHouse Keeper можно поддерживать в двух разных установках: в качестве обособленной инсталляции на отдельных серверах или как часть ClickHouse в виде Process Keeper — части бинарного файла ClickHouse clickhouse-server. Несмотря на то, что второй вариант кажется проще и дешевле (так как по сути Keeper уже встроен в ClickHouse), мы настоятельно рекомендуем использовать ClickHouse Keeper как отдельную инсталляцию, так как это повышает отказоустойчивость системы в целом.

<internal_replication>true</internal_replication>

#### ZooKeeper

Внутри Zookeeper все данные о кластере будут хранится в директории /clickhouse и в ней будет находятся следующее:

ddl — все DDL запросы, которые необходимо реплицировать и применять к репликам.
metadata — хранит часть информации о метаданных реплицируемой таблицы, например первичный ключ, ключ раздела и т. д;
columns — хранит информацию о полях реплицируемой таблицы;
block_numbers — хранит все разделы таблицы;
leader_election — используется для выбора мастера между репликами. При запросе сначала будет выбрана ведущая реплика, а затем на ее основе будет синхронизирована другая реплика. Несколько реплик могут быть лидерами одновременно;
log — каталог, служащий очередью задач для хранения операций над репликой таблицы;
blocks — хранит хэш-информацию блоков данных, записанных в таблицу за определенный период времени для дедупликации. Формат следующих дочерних узлов — partition_hash_hash;
mutation — каталог, служащий очередью задач для хранения операций мутации над реплицированной таблицей;
replicas/node_hostname — хранит информацию о репликах:
is_active — флаг, указывающий, активен ли инстанс или нет, если на сервере произошел сбой, узел не будет существовать, и будет добавлен заново после восстановления;
mutation_pointer — хранит следующую задачу в очереди мутаций, которую следует выполнить;
log_pointer — хранит следующую задачу в очереди журналов, которую следует выполнить;
is_lost — флаг, указывающий, устарела ли реплика, на основании того, обновлён ли указатель log_pointer, при этом 0 — нормально, -1 — устарела, а 1 — находится в процессе восстановления;
metadata — хранит информацию о метаданных таблицы, как и metadata выше;
columns — хранит информацию о столбцах таблицы;
parts — хранит информацию о партах таблицы, каждый блок информации содержит контрольные суммы и информацию о столбцах;
queue — временная очередь обработки


## Создание сущностей

В рамках MergeTree вы вставляете парты, а ClickHouse будет производить их вставку и слияние с другими партами в фоновом режиме, количество потоков для выполнения фоновых задач можно задать с помощью настройки background_schedule_pool_size.

DDL и TRUNCATE операции также координируются, если применены с ON CLUSTER. Таким образом, почти все операции вставки или изменения таблиц будут реплицироваться, кроме следующих:

CREATE table;
DROP table;
DETACH table;
ATTACH table;
ALTER TABLE FREEZE;
ALTER TABLE MOVE TO DISK;
ALTER TABLE FETCH;

Указанные выше операции являются локальными для инстанса и их необходимо выполнять на всех узлах кластера отдельно.

## Переход
Если в процессе эксплуатации появляется необходимость перейти с нереплицируемой MergeTree таблицы на её Replicated-версию, то есть два основных варианта:

1) Ручной: создать реплицируемую таблицу с таким же движком и структурой, после чего присоединить партиции через ATTACH одну за одной. Этот вариант работает на всех версиях ClickHouse.  

2) Автоматический: можно добавить флаг репликации в каталог таблицы и ClickHouse автоматически преобразует таблицу в реплицируемую. Эта функция была введена в версии 24.2.

На основе все вышеописанного можно заключить, что репликация в ClickHouse — это надёжная конструкция, не обладающая точками отказа. И в целом так оно и есть: по нашему опыту репликация в ClickHouse действительно надёжна и почти никогда не ломается. Почти…


# Важно обратить внимание на:

Keeper должен находится на отдельных серверах, желательно на SSD дисках 3,5 или 7 узлов;

Не стоит очищать Keeper вручную;

Делать бэкап информации из Keeper;

Систематически обновлять версию ClickHouse для возможности использовать все актуальные инструменты;


### Проблемы
1. При удалении контейнера, данные автоматически не реплицируются (имею ввиду что не создается база данных и таблицы)

Данные автоматически реплицируются если разница между данными не велика 

Когда реплика была полностью потеряна (в данном примере это clickhouse01). Или данные с неё были полностью удалены, *нужно создать базу и таблицы руками*.
После создания базы, воссоздание потерянных таблиц может быть проблематично, это связанно с ошибкой 
`Code: 253. DB::Exception: Replica /clickhouse/tables/company_cluster/01/events/replicas/clickhouse01 already exists `
в которой говорится что (потерянная)реплика уже существует, хотя по факту её нет, так происходит из-за того, что на второй реплике (рабочей), сохранилось упоминание о ней.
Для устранения этой ошибки, нужно зайти на (рабочую) реплику и выполнить:
1. Проверить точно ли текущая реплика является клоном потерянной
`SELECT * FROM system.replicas WHERE table = 'events' FORMAT Vertical;`

Премерный вывод
```sh
database:                    company_db
table:                       events
engine:                      ReplicatedMergeTree
is_leader:                   1
can_become_leader:           1
is_readonly:                 0
is_session_expired:          0
future_parts:                0
parts_to_check:              0
zookeeper_path:              /clickhouse/tables/company_cluster/01/events
replica_name:                clickhouse01
replica_path:                /clickhouse/tables/company_cluster/01/events/replicas/clickhouse01
columns_version:             -1
queue_size:                  0
inserts_in_queue:            0
merges_in_queue:             0
part_mutations_in_queue:     0
queue_oldest_time:           1970-01-01 03:00:00
inserts_oldest_time:         1970-01-01 03:00:00
merges_oldest_time:          1970-01-01 03:00:00
part_mutations_oldest_time:  1970-01-01 03:00:00
oldest_part_to_get:          
oldest_part_to_merge_to:     
oldest_part_to_mutate_to:    
log_max_index:               1
log_pointer:                 2
last_queue_update:           1970-01-01 03:00:00
absolute_delay:              0
total_replicas:              2
active_replicas:             2
last_queue_update_exception: 
zookeeper_exception:         
replica_is_active:           {'clickhouse01':1,'clickhouse02':1}
```

2. Удаление метаданных реплики
`SYSTEM DROP REPLICA 'clickhouse01';`

После выполнения этой команды, снова проверяем состояние реплики пердыдущей командой
```sh
zookeeper_exception:         
replica_is_active:           {'clickhouse02':1}
```
3. Создать утерянную таблицу, аналогичным образом как была создана таблица на реплике.
Если нет рядом информации как была создана таблица, можем посмотреть на реплике, используя команду
`SHOW CREATE TABLE events;`

На основе полученного результата, создаем таблицу.
Проводим эту операцию с каждой таблицей.

*После пересоздания таблицы, данные автоматически реплицируются в неё*

4. Провести данную операцию для каждой таблицы.

# Шардирование

В некоторых случаях, нет необходимости создавать избыточные данные, репликации (в силу различных особенностей, такая необходимость может быть). Но нужно обеспечить горизонтальное масштабирование.
В данном случае прекрасно подхиодит решение только шардирования.

Но есть и ряд недостатков, которые с "коробки" могут вызвать определенные трудности при добавлении,удалении,потере одного из шардов.

Основной из них:
При потере шарда, выборка, вставка данных в доступные шарды невозможна!
Для этого, нужно добавить настройки

```tml
<code>code</code>
```

Но, это решает только одну проблему, это выборка данных **НО** при вставке данных, у нас возникает проблема:
Clickhouse работает так:
у него есть 3 шарда, когда идет вставка 3-х строк, он (по очереди) вставляет их в каждый шард (строка на шард). В случае, когда 1 из шардов не доступен, вставка данных просто проигнорируется, это значит что мы теряем данные!!!

Для устранения этой ошибки, есть несколько путей.
Сохранять временно на диск данные (до момента заполнения выделенного места или до восстановления шарда)

Возможно, есть возможность записывать на доступные шарды данны игнорируя недоступный (нужно исследовать)

### Изменение кол-ва шардов
В ClickHouse шардирование осуществляется на уровне распределённых таблиц и серверов, что позволяет распределять данные по различным узлам кластера. Для добавления нового шарда или удаления существующего нужно учитывать несколько аспектов, таких как топология кластера, стратегия шардирования и поведение распределённых таблиц.

### 1. **Добавление нового шарда**
Чтобы добавить новый шард в ClickHouse:

1. **Добавление нового узла:** Запустите новый ClickHouse сервер и убедитесь, что его конфигурация соответствует остальным узлам в кластере (например, у него установлен `keeper` для синхронизации данных).
2. **Обновление конфигурации кластера:**
   - Добавьте новый шард в конфигурацию ClickHouse (`config.xml`) на всех узлах кластера, в секции `<remote_servers>`. Например:

     ```xml
     <remote_servers>
       <my_cluster>
         <shard>
           <internal_replication>true</internal_replication>
           <replica>
             <host>shard1-node1</host>
             <port>9000</port>
           </replica>
         </shard>
         <!-- Новый шард -->
         <shard>
           <replica>
             <host>shard5-node1</host>
             <port>9000</port>
           </replica>
         </shard>
       </my_cluster>
     </remote_servers>
     ```
### 2. **Автоматизация создания таблиц**
Чтобы автоматизировать процесс создания таблиц на новом шарде, можно использовать следующие подходы:

#### 2.1. **Скрипт на основе SQL-запросов**
Можно написать скрипт на Python или Bash, который:
1. Получает список всех таблиц на существующих шардах.
2. Автоматически создает аналогичные таблицы на новом шарде.

Пример на Python с использованием библиотеки `clickhouse-driver`:

```python
from clickhouse_driver import Client

# Подключение к существующему шару для получения списка таблиц
client = Client(host='existing-shard-host', user='defauЗдесь:
- `'/clickhouse/tables/{shard}/my_table'` — путь в ZooKeeper для таблицы.
- `{replica}` — имя реплики, которое будет заменено на имя текущего узла.

### 2. ***

мер, чтобы создать таблицу `my_table` на всех узлах кластера:

```sql
CREATE TABLE default.my_table ON CLUSTER 'my_cluster'
(
    id UInt32,
    name String
)
ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/my_table', '{replica}')
ORDER BY id;
```
lt', password='password')
tables = client.execute("SHOW TABLES")

# Подключение к новому шарду
new_shard_client = Client(host='new-shard-host', user='default', password='password')

# Создание таблиц на новом шарде
for table in tables:
    create_query = client.execute(f"SHOW CREATE TABLE {table[0]}")[0][0]
    new_shard_client.execute(create_query)
```
#### 2.2. **Выполнить создание руками**
Команда `ON CLUSTER` в ClickHouse позволяет выполнять SQL-запросы на всех узлах кластера. Если вы добавили новый шард и хотите создать таблицы на нем, то нужно выполнить команду `CREATE TABLE` с параметром `ON CLUSTER`, и ClickHouse автоматически выполнит команду на всех шардах и репликах кластера, в том числе на новом.

```sql
CREATE TABLE IF NOT EXISTS company_db.events ON CLUSTER '{cluster}' (
    time DateTime,
    uid  Int64,
    type LowCardinality(String)
)
ENGINE = MergeTree()
PARTITION BY toDate(time)
ORDER BY (uid);
```
Этот запрос создаст таблицу `events` на всех узлах, включая новый шард.

### 3. **Копирование данных на новый шард**
После создания таблиц на новом шарде можно скопировать данные с других шардов:

1. Использовать запрос `INSERT INTO ... SELECT ...` для перемещения данных.
2. Либо настроить распределенную таблицу так, чтобы она автоматически учитывала новый шард при записи новых данных.
4. **Изменение распределённой таблицы:**
   - Обновите конфигурацию распределённой таблицы, чтобы включить новый шард. Это можно сделать через `ALTER TABLE`:

     ```sql
     ALTER TABLE <distributed_table_name> ADD SHARD 'shard5';
     ```

5. **Распределение данных:** Теперь можно начать перемещать или пересоздавать данные на новом шарде. Прямого метода автоматического перемещения данных нет, но можно использовать `INSERT INTO ... SELECT ...` для копирования данных из старых шардов на новый.

### 2. **Удаление шарда**
Для удаления шарда из кластера:

1. **Измените конфигурацию:** Удалите соответствующий шард из конфигурации `config.xml` в секции `<remote_servers>` на всех узлах.
2. **Удалите или перенесите данные:** Если данные на удаляемом шарде необходимы, переместите их на другие шарды с помощью SQL-запросов.
3. **Удалите шард из распределённой таблицы:**

   ```sql
   ALTER TABLE <distributed_table_name> DELETE SHARD 'shard_to_remove';
   ```

4. **Перезапуск ClickHouse:** Перезапустите все узлы ClickHouse, чтобы изменения вступили в силу.

### 3. **Поведение при нехватке места на одном из шардов**
Если на одном из шардов заканчивается место, ClickHouse будет продолжать записывать данные на другие доступные шарды. Однако на шарде, на котором нет свободного места, любые операции записи будут завершаться ошибками. Поведение системы зависит от стратегии шардирования:

- **Если используется взвешенное шардирование (`weight`):** Данные будут записываться на те шарды, у которых еще есть место. Задача распределения данных между шардовыми узлами будет выполнять балансировку, исходя из веса.
- **Если нет взвешивания:** Запись на полностью заполненный шард будет завершаться ошибкой, но система продолжит функционировать на других шардах.

Для мониторинга свободного места на шардах можно использовать такие SQL-запросы:

```sql
SELECT
    shard,
    free_space,
    total_space,
    free_space / total_space * 100 AS free_space_percent
FROM system.disks
```

### 4. **Рекомендации по управлению шардами**
- Следите за распределением данных, чтобы избежать дисбаланса между шардами.
- Внедрите мониторинг свободного места и состояния шардов для предотвращения ситуаций с нехваткой ресурсов.
- Если возможен сценарий с нехваткой места, стоит использовать стратегию репликации, чтобы данные автоматически распределялись между репликами, что снижает вероятность отказа при записи.


### 4. **Использование утилиты `clickhouse-copier`**
ClickHouse предоставляет утилиту `clickhouse-copier`, предназначенную для копирования данных между шардовыми узлами. Эта утилита может автоматизировать процесс миграции данных между шардами, но она требует настройки конфигурационного файла с указанием источника и назначения данных. Пример конфигурации:

```xml
<job>
    <task>
        <table>my_table</table>
        <shard>
            <replica>
                <host>source-shard-host</host>
                <port>9000</port>
            </replica>
        </shard>
        <destination>
            <shard>
                <replica>
                    <host>new-shard-host</host>
                    <port>9000</port>
                </replica>
            </shard>
        </destination>
    </task>
</job>
```

Запустите копирование:

```bash
clickhouse-copier --config config.xml
```

### Итог
Для полного добавления нового шарда в кластер и создания идентичных таблиц следует:
1. Использовать `ON CLUSTER` для создания таблиц на новом шарде.
2. При необходимости — использовать `clickhouse-copier` для миграции данных.
